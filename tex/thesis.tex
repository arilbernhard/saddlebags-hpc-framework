% Use the uit-report.cls to style the document
\documentclass{uit-report}



%%%% PREAMBLE ---- here you can add packages if you need extra functionality %%%%

% Provides the \lipsum command to create "Lorem Ipsum" sample text.
\usepackage{lipsum}
\usepackage{listings,multicol}
\usepackage{varwidth} %for the varwidth minipage environment
\usepackage{pdfpages}

\setcounter{tocdepth}{1}





\begin{document}
\includepdf[pages=-]{front.pdf}
\title{Saddlebags: An Object-Oriented Parallel Computing Framework for C++}
\author{Aril Bernhard Ovesen}
\date{June 1, 2018}


% Saddlebags - A High Performance Computing Framework
%An object-oriented general-purpose parallel framework with UPC++

\newpage\
\pagestyle{empty}

\mbox{}

\pagebreak
\pagenumbering{roman}
\pagestyle{firststyle}
\section*{Abstract}
The volume of data generated by web services and scientific tools is increasing more rapidly than many currently available high performance computing tools are able to keep up with. There is a need for novel data abstractions and parallel computing frameworks that can handle workloads from the fields of both high performance computing and big data analysis. This thesis presents Saddlebags, a high performance computing framework built on UPC++. Saddlebags aims to achieve high performance by coupling data and computing task together to exploit data locality. Principles such as fine grained remote data access and explicit data movement is inspired by high performance computing systems, while concepts such as implicit data distribution and fault tolerance is inspired by big data systems. Saddlebags can be used on individual nodes and on compute clusters, and achieves favorable performance when compared to other modern systems on both platforms.


\newpage
\section*{Acknowledgements}
%I wish to extend my gratitude to several people.

%To Phuong H. Ha, my advisor for the last two semesters, for his input and \\expertise. To my good friend Tor-Arne for his humor, and knowledge of pretty much every scientific topic. To my super-girlfriend and biggest fan, Johanne, for her never-ending love and support.

\newpage
\tableofcontents
\pagebreak
\listoffigures
\pagebreak
\listoftables
\lstlistoflistings
\pagebreak
\pagestyle{empty}

\mbox{}

\newpage
\pagenumbering{arabic}
\pagestyle{secondstyle}
\setcounter{page}{1}



\chapter{Introduction}


%  State of high performance computing
%  Shared memory and message passing
%  Role of different programming models/abstractions
%  Data centric computation
%  UPC++

%Scientific applications and engineering research are undergoing a transformation
%At the end of Dennard Scaling around 2004, transistor clock frequencies could not be raised without increasing the total circuit power consumption.
%Introduced multicore, manycore and accelerator-based computing 
%The big data revolution 
%Analysis, management, and collection of large volumes of data makes up the field of data-intensive science. 

%The core of high performance computing is writing efficient and parallel applications, which is the concern of researchers from an increasing range of disciplines, from physics and chemistry to medicine and geology. While the available hardware is being steadily improved,  \cite{engineering}. 


High performance computing is becoming of increased importance for scientists and researchers of a large range of disciplines, from physics and chemistry, to medicine and geology. At the very core of high performance computing lies the ability to write efficient and parallel software~\cite{engineering}.

The U.S Department of Energy reported in 2013 through the Advanced Scientific Computing Advisory Subcommittee that the data volumes produced by scientific experiments and simulations were increasing more rapidly than the currently available algorithms, computing systems and workflows were able to keep up with. They also reported a need for innovations in high performance computing and data-intensive science to address challenges with contemporary systems related to concurrency, data movement, and energy efficiency~\cite{subcommittee}.

Big data frameworks such as Apache Hadoop~\cite{hadoop}, Spark~\cite{spark} and Flink~\cite{flink} can provide simple programming interfaces for distributed algorithms, with the underlying assumption that distribution of data and computing tasks can be handled by the system without programmer input. Programmers are given functionality to access data without explicitly managing communication between nodes. These systems are often reliant on coarse grained operations, which in many cases can produce inefficient programs, and limit the programmability of a framework~\cite{husky}\cite{piccolo}. Many data analytics operations are often orders of magnitude slower when implemented in a data analytics framework like Spark, than when written with native high performance computing tools such as MPI~\cite{bridgingthegap}. However, with a simple programming model, support for numerous data formats, underlying fault tolerance and automated data distribution, Spark remains an attractive option for distributed big data workloads~\cite{sparkarticle}. 

As many scientific applications depend on processing high volumes of data, problems of high performance computing and big data analytics are converging. As the amounts of data increase, so does the importance of developing more efficient analytical tools~\cite{hpcabds}. Features from big data systems can be incorporated into HPC frameworks, to create programming models that are intuitive and higher-level, yet expressive and performant. This thesis presents Saddlebags, a framework that attempts to strike a balance between the two paradigms of parallel computing systems. Inspired by big data frameworks, Saddlebags supports automatic distribution of data and computational tasks, and removes the need for specialized communication routines based on physical location. Inspired by HPC frameworks, Saddlebags allows fine grained memory access of distributed data, and leaves data movement during computations explicit and up to the programmer.

Saddlebags is implemented in C++. It uses parallelization and communication implementations from UPC++, a Partitioned Global Address Space library. With UPC++, a program has a static number of threads, each of which is given the affinity of a unique private memory segment. While data can be shared across segments, a memory segment is mainly accessed by the thread it has affinity to. Saddlebags applications can be used either on a single node or across multiple nodes within a compute cluster, using Ethernet or InfiniBand for network communication.

%while still allowing fine grained memory access, 


%Advantageous programming models from big data systems can be incorporated into 


%By incorporating concepts of simple programming models from big data systems, into HPC frameworks typically known for complex, we can develop tools that can solve problems that depend on both big data processing and performant computation.

%As the importance of advanced big data analytics

%Highly performant communication or computation a


%Designing efficient, yet intuitive programming models for distributed computing and data analytics seems to .
\newpage
\section{Contributions}
In brief, we make the following contributions

\begin{itemize}
	\item Saddlebags, a high performance computing framework and programming model that borrows features from several other HPC frameworks and big data systems, with the aim of supporting workloads of different classifications.
	\item With the implementation of this framework, we use the UPC++ PGAS library as a communication layer for more advanced abstractions. To our knowledge, this is the first general-purpose HPC framework built with UPC++.
\end{itemize}






%A framework for implementing distributed algorithms and applications, created in and for C++, using UPC++, a library that provides PGAS mechanisms for communication. The framework provides an alternate programming model for distributed algorithms, in which a computational task, its data and its communication routines, is represented as a single object. 

% Distribution based on 


\section{Outline}
This thesis is organized as follows. Section 2 presents background knowledge required to understand the rest of the thesis, including a summary of related work. Section 3 presents principles of Partitioned Global Address Space programming, and provides an overview of the UPC++ library. Section 4 describes the design of Saddlebags. Section 5 describes details of the implementation. Section 6 then covers an experimental evaluation of the system. Section 7 discusses topics related to Saddlebags that has not been covered in other sections. Section 8 describes potential future work and improvements to the system. Section 9 concludes the thesis.

\newpage
\mbox{} \pagebreak
\chapter{Background}
This chapter will present an overview of the background behind this thesis. The purpose of this is to provide a technical and historical context of the work that will be presented in the following chapters. A key concept that will be referenced throughout the thesis is High Performance Computing. This term refers to the technical field of working with super computers to solve complex computational problems. This involves the development of parallel algorithms, distributed systems and computing platforms, to meet a demand for high processing speeds~\cite{hpc_def}.

%  History + State of distributed computing / high performance computing
%  Message passing vs PGAS
%  Data intensive computing
%  Coupling of Data and processing task -> computation close to data
%  benefits of Abstractions + introduce data/task coupling as a potential 
%  Object oriented high performance computing: object contains data AND task
%  Distributed objects
%  
\section{High Performance Computing}
%The field of high performance computing has experienced two major disruptions during the 21st century.

Software performance gains from increased frequency in single processing units reached its limits many years ago, and multi-core systems has been prominent in personal computing since. Up until the early 2000s, the speed at which computers could execute sequential programs would steadily increase as clock speed and semiconductor fabrication improved \cite{sutter_larus_2005}. Heat dissipation and energy consumption caused development of the maximum achievable CPU frequency to stall, effectively freezing the number of tasks that can be completed within a certain time frame on a single processing unit \cite{diaz_munoz-caro_nino_2012}. The answers to these problems were multi-core and many-core systems, which bypasses the aforementioned performance wall by utilizing several processing~units.


Within the field of high performance computing, multi-processor systems were not a novelty. Using networks of workstation computers for parallel computation was already an attractive alternative to traditional supercomputers, partly because new processor technology could easily be incorporated without replacing the entire system \cite{wilkinson_allen_2005}. While the field of parallel programming was relevant in high-end scientific applications before halt in clock speed development and the following concurrency revolution, homogeneous distributed systems now dominate the field of high performance computing. The largest and fastest supercomputers in the world today contain millions of processor cores \cite{top500}.


Multi-core and many-core architectures achieve parallelism through explicit task distribution and scheduling, managed by the programmer. Exploiting parallelism is often an application-specific issue, and can pose several challenges. In the fields of distributed- and high performance-computing, aiming for scalability further increases the complexity of the programming task. Several programming models have been devised to support development of distributed, parallel applications, and the current leading models can be clustered into different language paradigms: \emph{shared memory }and \emph{message passing} \cite{pgas_languages}. On top of these communication models, abstractions can be applied to hide complexity from the programmer in order to reduce development time and ease the debugging process. 

\section{Programming models}

There exists a perception that abstraction carry an inherent performance penalty, but abstractions can also be powerful tools in high performance computing \cite{mccandless_lumsdaine_1997}. For example, computing at a large scale can introduce issues relating to handling large amounts of data, scaling to a large or arbitrary number of computation nodes, or developing algorithms that utilizes communication routines and synchronization.

Identifying common patterns in different problems allows for development of frameworks and libraries that are optimized to handle classes of problems matching the same patterns. For example, the MapReduce framework hides significant programming overhead of problems that can be expressed within semantics provided by the framework and the programming model it represents. In its most primitive, data is supplied to the \emph{Map} function and a function to execute on the data is supplied to the \emph{Reduce} function. The MapReduce programming model is designed for algorithms and problems that can be expressed using these two functions, and the MapReduce framework is implemented to provide functionality that can be applied generally to problems within that programming model.

In addition to implementing communication routines and data distribution logic, a framework can supply scalability and fault tolerance, due to the information about the application that is implied by the programming model it is utilizing. Other examples of systems specialized in specific programming models include Dryad \cite{dryad}, which models an application's data flow into directed acyclic graphs.

\section{Data intensive computing}
% asdf HPC vs big data
Many scientific fields are driven by collection and analysis of large amounts of data, and the workflows of many scientific communities based on either data analysis or computing tasks have in recent years converged towards solving similar problems \cite{bigdataconvergence}. Terms like \emph{data deluge} and \emph{Big Data revolution} are used to describe the volume of data generated by modern scientific tools and web applications. For example, large data volumes are generated by instruments such as high resolution microscopes \cite{microscopy} and gene sequencers \cite{mapreduce_locality}, or online enterprises such as Google, Facebook and Amazon. In fields of research, applications can use workflows that interweave simulations and data analysis, or that utilize both data generation and data consumption. 

Historically, data intensive sciences and computational science has been considered separate scientific fields, which has required different classes of applications and platforms \cite{Hey2009JimGO}. It has also been proposed that data analysis and intensive computation can be considered different steps in a generalized scientific process, in which data analysis is the fundamental observation of reality, and computation is used to deduct conclusions from theories built on the available data \cite{honavar2016accelerating}. Such an understanding of a contemporary scientific method would incur that the field of computer science, traditionally aimed at solving computational problems given a problem and a set of input data, would be expanded to data gathering and exploration as well, not as an end goal, but as a step in a larger process.

A view of unified challenges in data intensive computing and high performance computing can formulate opportunities that benefit both fields. This can apply for two classes of workloads. Firstly, problems that require interweaved tasks of data analysis and computations will require unified application ecosystems and platforms. Secondly, problems that are defined with discrete steps of either data consumption and data analysis can benefit from the improvement in technology from both fields and their applications separately. In practice, the use of multiple platforms for solving a single pipelined problem can incur performance penalties due to differences in data structure and problem formulations~\cite{husky}. This penalty may grow larger as data volumes increase.

The convergence of workloads in high performance computing and big data analysis has caused many to conclude that there is a need for new applications, platforms, or data abstractions that can be applied to problems of both classes. Work has gone into merging existing big data frameworks with high performance computing technology~\cite{bridgingthegap}\cite{gittens2016matrix}, and devising new abstractions and data structures \cite{yang2017best}. Detailed descriptions of problems that benefit from merging workflows from big data analytics and high performance computing are given in \cite{subcommittee} and \cite{bigdataconvergence}.

\section{Related work} \label{section:relatedwork}
Presentations of several modern high performance computing frameworks and big data systems follow. These systems are described here because they in-part make up the current state of the art high performance computing techniques and programming models. The conflicting principles of compute-oriented and data-oriented computation are exemplified in the features of the systems that are in use today.

\subsection{Pregel}
Pregel \cite{pregel} is a system for distributed graph processing, first presented in 2010. Pregel was designed to scalably process large graphs created by web services. The workflow of Pregel involves distributing the vertices of a graph across nodes in a cluster, and performing independent computational tasks on each vertex. These tasks are performed iteratively, meaning that individual vertices may execute tasks in parallel, but the beginning and end of an iteration is synchronous. In Pregel, these iterations are called \emph{supersteps}. The execution model of supersteps is equivalent to a Bulk Synchronous Parallel execution model, from which Pregel claims inspiration. 

Vertices are distributed across a cluster by partitioning the graph into subgraphs, consisting of vertices and edges, each of which is assigned a compute node. Each vertex in the graph will execute some user-defined work task, and may as a result of this task vote to end the graph processing. The system will execute supersteps until every vertex is in agreement of ending the computation. Vertices can communicate by sending their value to other vertices, and \emph{aggregators} provide a form of communication through global monitoring of all data. Messages that are received in one superstep is available in computation during the next superstep.

A node is classified as either the master or a worker. A worker contains vertices, and executes the computational tasks associated with them. A master coordinates the activity of the workers, including handling data input, output, and global barrier synchronization. The master is also responsible for detecting faults in the active workers, initiating fault recovery mechanisms that are implemented through checkpointing.

Pregel claims better performance than MapReduce-based workflows through passing messages directly to the nodes containing relevant vertices. This is in contrast to expressing a graph as a chain of application states, where the entire graph must be re-created and re-distributed for each iteration.

\subsection{Spark}
Apache Spark \cite{spark} is a framework for big data applications, initially released in 2014. Spark aims to provide a unified programming model for different distributed workloads that handles large amounts of data. Spark is based around Resilient Distributed Datasets (RDDs), an abstract data type that implicitly distributes data across nodes and provides fault tolerance through re-creation of the data on faulty nodes. Spark provides an extension of the MapReduce model, that aims to support iterative and streaming operations more efficiently through data-sharing across multiple stages of parallel workloads. This is in contrast to earlier MapReduce-systems that, by using a distributed file system for storage, replicated data across all nodes in-between all stages of computation.

RDD operations are coarse-grained and applies to the entire dataset per execution step. This means that operations applied to an RDD can easily be logged, and the entire dataset can be re-created in case of faults. Computations with RDDs can be represented as a directed acyclic graph (DAG), in which the RDD moves from one state to the next, by applying transformations on the entire data structure. RDDs are re-created by replaying the DAG.

Extending the data sharing of the MapReduce model increases its generality. Iterative algorithms, stream processing, and relational queries are among the programming models supported by Spark.

The execution model of RDD operations is more complex than traditional data structures. RDD transformations are lazily evaluated, meaning that they are not performed until the result of the transformation is required by an I/O operation. When writing Spark programs, variables of an RDD type does not necessarily represent a materialized object, and unless the programmer explicitly states otherwise, RDDs are only used for a single operation and will be recomputed the next time the same variable is used for another operation.

\subsection{Husky}
Husky \cite{husky}\cite{husky_website} is a framework for large scale data mining and research of distributed algorithms. It was first presented in 2015. Husky is designed around the observation that high performance can generally be achieved through controlling low-level mechanisms, at the expense of development time. Husky's design aims to provide a system for expressing advanced data interactions, while being cautious about development costs.

The programming model of Husky is based around two main concepts: firstly, distribution of programmer-defined objects that contain some data element, and secondly, programmer-defined computational tasks that can be executed on these objects. This distribution of data and tasks is similar to the Pregel definition of vertices. Objects can communicate with each other using a push-pull model, and functions executed on data can choose to parse through received pushes. Objects are stored in lists, and multiple lists can exist, potentially containing different object types. Computational tasks can be executed on all objects in a list in bulk, and communication is guaranteed to have completed in-between tasks. This classifies Husky's execution model as Bulk Synchronous Parallel. Synchronization mechanisms can also be disabled.

Husky implements a Master-Worker architecture, where workers store data and perform computations, while the master is responsible for synchronization and fault tolerance. Objects are distributed across workers through a programmer-defined partition function. All objects are assigned an identifier, which can be used to implement partitioning logic. Husky's programming model can be extended to imitate other models, such as Pregel's vertex distribution, MapReduce, and machine learning parameter servers.

Fault tolerance is implemented through checkpointing. As the execution model of a Husky program (when synchronization is not explicitly disabled) is iterative, it is easy to recover from a state in-between iterations, as all messages has been delivered and no computation is in progress. Workers will routinely save their state to disk between iterations, and the master checks if all workers are alive through heartbeats. 

The Husky paper shows experimental results in which Husky compares positively to other systems including Spark in several different workloads. With bulk data movement workloads, Husky explains superior performance through fine-grained data access. With graph analytics, Husky explains superior performance through optimized combination of communication messages, which reduces network bandwidth usage. 

%\section{Other work}
% Legion
% DataMPI
%% PGAS energy evaluation?

\newpage
\chapter{PGAS programming and UPC++}
Traditionally, parallel programming libraries (and the field of High Performance Computing) has been clustered into either shared memory models and message passing models \cite{pgas_languages}. Shared memory models provide an abstract memory space that is available for access across several concurrent compute threads. Message passing models, such as MPI \cite{MPI}, give a programmer control of the flow of execution by conceptually bundling most of the communication into a message abstraction. This approach can be used to achieve high performance; the programmer can reason about the cost of communication, and messages can be delivered between compute nodes to scale a program to multi-node systems with predictable use of bandwidth.

%add something about how shared memory is easier?

Programming languages that achieve parallelism through a shared memory abstraction that provides memory access to several compute threads concurrently can be classified as Global Address Space (GAS) langauges. Communication between threads is achieved through reading and writing shared data, rather than explicit message passing \cite{gasnet_description}. Partitioned Global Address Space (PGAS) languages aim to provide a hybrid approach between previous parallelization paradigms, by utilizing the programmability advantages of the shared memory model, and the communication efficiency of message passing.

\section{GASNet}
GASNet is a communication API for GAS/PGAS languages that aims to provide a portable communication layer to systems that would otherwise be platform specific or network implementation dependent. Portability is achieved through translating PGAS programs to an intermediate C representation, which can be compiled into a platform specific application using the system's standard C compiler \cite{gasnet_description}.

GASNet is distributed with several API implementations with support for different underlying network architectures, called \emph{conduits} \cite{gasnetreadme}. This means that systems built on GASNet can be implemented to utilize different communication interfaces, including, among others, MPI, shared memory, and InfiniBand verbs.

\section{Unified Parallel C}
Unified Parallel C, referred to as UPC, is a superset of the C programming language for writing SPMD programs, utilizing a global address space paradigm for parallelism. A complete UPC specification was first standardized in 2001, and several UPC compilers have been under development since, including GNU UPC, which is an extension of GCC \cite{web_gnu_upc}, and Berkeley UPC, which is dependent on GASNet \cite{web_berkeley_upc}.

The address space of a UPC program is partitioned into several logical fragments, with each concurrent thread being mapped to one fragment. With this model, each thread has affinity to its own individual memory region, which makes it possible to exploit data locality \cite{evaluation_of_upc} and utilize parallelism with the same language constructs.

Handling data in a memory partition mapped to a single thread makes it implicit that data is being accessed by a thread that has affinity to the given partition. Memory can also be explicitly distributed across several threads, using shared global memory, which disregards any thread affinity and is logically separated from the aforementioned thread-mapped memory partitions.

The shared memory abstraction of UPC is provided regardless of underlying hardware structure. Partitioned memory segments can be mapped to individual compute nodes in a cluster, utilizing data locality by performing computation on private data close to the location where it is stored, while still providing a shared memory abstraction for communication and programmability. The original UPC language specification states that a driving principle behind UPC is that parallelism and remote memory access should not obfuscate the resulting program; programmers should themselves utilize the language constructs to design programs and data structures, instead of being provided with solutions to specialized tasks. Additionally, the specification claims that a shared memory programming model is attractive for users of distributed memory systems \cite{upc_language_specification}. These two concepts can in part make up the motivation behind the UPC language.



\section{UPC++}
UPC++ is a PGAS extension to the C++ programming language. It is developed at University of California, Berkeley and saw the release of its 1.0 version in September 2017. UPC++ is implemented as a library extension, due to the complexity of developing a separate C++ compiler \cite{zheng_kamil_driscoll_shan_yelick_2014}. The library heavily utilizes C++ template programming \cite{web_cpp_template} for PGAS mechanisms, which allows the creation of library functions that use generic variables that can be adapted to different types. 

\subsection{Programming model}
UPC++ follows the SPMD model and like UPC utilizes threads with affinity to memory partitions. While UPC++ allows the use of C++ functionality such as objects and lambdas, it still aims to provide high performance and explicit communication by leaving much responsibility regarding concurrency and data movement to the programmer.

Each UPC++ partition has access to its own private memory segment, and is identified by its rank, an integer that can range from zero to the total number of partitions. Listing \ref{lst:helloworld} demonstrates a simple program that is started with 4 threads, each printing their rank, illustrating the Single Program Multiple Data property of UPC++ programs. Ranks can communicate through a logical shared memory segment, by utilizing UPC++ communication functionality, including global pointers, remote procedure calls (RPCs), and distributed objects. The number of ranks is static during execution; the total rank count is specified when launching a program and cannot be modified during runtime.


\begin{lstlisting}[label={lst:helloworld}, float, caption="Hello World"-program in UPC++ and its output, frame=tlrb, captionpos=b, language=c++, showstringspaces=false]
void main()
{
  upcxx::init();
  cout << "Hello world! " << upcxx::rank_me() << endl;
  upcxx::finalize();
}

-----------------------------------------------------------
Output:
Hello world! 0
Hello world! 2
Hello world! 3
Hello world! 1
\end{lstlisting}



Through communication or internal operations, a single thread may consist of several independent tasks that need access to computing resources for the program to continue its execution. In order to keep communication data movement predictable, a programmer will need to yield control of execution. This means that operations that require communication, such as RPC or global memory accesses, are separated from the main thread of execution and will not expend computing resources until explicitly granted permission to do so. Remote and asynchronous tasks are exposed to the caller through \emph{future} objects, which will change their internal state when the task has completed. This allows the caller to perform operations dependent on the completion of remote tasks or retrieval of remote data, and effectively communicate between ranks.

The 1.0 release of UPC++ introduced the concept of \emph{distributed objects}, which are objects defined by a generic type within UPC++ with the same identifier across all ranks. This means that data stored in the private memory segment of a partition can be referenced by a different rank in RPC calls. The data within the object is not implicitly distributed, but the name of the variable is. Local contents of the distributed objects can as such be explicitly distributed to any rank.

\subsection{Progress}
Outstanding asynchronous operations are not completed automatically, because the program consists of a static number of threads. In order to advance the internal state of UPC++ or complete RPC calls, computing resources are yielded from the main thread of execution via the \emph{progress()} function. This concept puts the responsibility of frequently progressing UPC++ state on the programmer's shoulders. The goal of this is to provide visibility of resource usage and parallelism, the latter being important to achieve interoperability with libraries and software packages that would otherwise restrict the use of multi-threading. Being cautious about where to progress RPCs relieves the needs for concurrency control such as mutexes, and implies atomicity in partition-internal operations of basic data types. More complex data structures that are not designed for concurrent access can be safely referenced in RPCs across partitions as long as the \emph{progress()} function is used only between atomic accesses.

\begin{lstlisting}[label={lst:distribobj}, float=t,frame=tlrb, caption={Communication through RPC, referencing a distributed object of an arbitrary C++ class. Rank 1 retrieves and prints \emph{myProperty} from rank 2's instance of \emph{myObj}.}, captionpos=b, language=c++, showstringspaces=false]
int main(int argc, char *argv[])
{
  upcxx::init();

  auto myObj = upcxx::dist_object<myClass>(myClass());
  upcxx::barrier();

  if(upcxx::rank_me() == 1)
  {
  
    auto f = upcxx::rpc(2, [](
      upcxx::dist_object<myClass> &my_dist_obj])
    {
      return (*my_dist_obj).myProperty;
    }, myObj);

    f.wait();
    std::cout << f.result() << std::endl;
    
  }

  upcxx::barrier();
  upcxx::finalize();
} 
\end{lstlisting}

\subsection{Remote Procedure Calls}

RPCs can be used for both modifying and copying data in the private segment of another rank's memory. The future objects returned by UPC++ RPCs can be used to indicate completion of the remote task, and communicate return values. A program that is dependent on data from a different rank, can halt advancement of the main thread by using the \emph{future.wait()} function, during which progress is attempted until the given future indicates that its corresponding remote operation has completed. This is not possible to achieve in tasks executed by progressing -- futures can only change state during progress, and progress-callbacks cannot themselves initiate further progress of other tasks. This concept provides a clear distinction between execution via the main thread of a rank, and execution via user-initiated progress of the same rank.

Listing \ref{lst:distribobj} demonstrates communication between two ranks using RPCs, distributed objects, and futures. A distributed object of an arbitrary class is initiated on all ranks, and a RPC is invoked to retrieve a property of the instance of the distributed object $myObj$ local to rank 2, back to rank 1. A barrier is used to ensure that the identifier of the distributed object is available on all ranks before it is referenced in the following RPC. The remote procedure is expressed with a lambda function, an unnamed function object. The RPC returns future $f$, which will hold a copy of the remote $myProperty$, the return value of the lambda function passed to the UPC++ $rpc$ function. The return value of the lambda will be available as \emph{f.result()} when the function has been executed, which is indicated by returning from \emph{f.wait()}. 

A distributed object is used in this example because it can be passed as a reference to the RPC on the sender's side, and it will be translated to the local copy of the distributed object with the same name on the receiving side. Non-distributed objects can only be passed as copy through RPCs, so distributed objects must be used for retrieving or modifying properties of remote objects. Within the RPC, the distributed object can be de-referenced to access properties of the underlying class. This is exemplified in listing \ref{lst:distribobj} through the access of $myProperty$.


\subsection{Global pointers}
Basic built-in C++ datatypes can be transmitted between partitions as RPC arguments or return values, by passing them as a copy. To communicate pointers to other partitions, the shared memory segment must be utilized. UPC++ can create a pointer to memory allocated in the shared memory segment, and copy local data to this location. The global pointer can be passed to other partitions through RPCs. Accessing global memory from a partition other than the one that initiated the pointer has to be done through asynchronous operations. UPC++ provides functions for remote puts and gets, which require internal progress on the partition that is hosting the shared memory in order to complete.


\newpage
\mbox{} \pagebreak
\chapter{Design} \label{chapter:design}
This chapter will cover the design and architecture of the system, outlining its different components, its programming model, and data flow. Section 4.1 explains the core concept and design philosophy of Saddlebags, Section 4.2 explains the programming model, while Section 4.3 outlines the architecture and data flow.

Saddlebags is a framework for parallel computing, aimed at applications and algorithms that can be implemented in C++. It uses UPC++ for thread spawning, communication, and data distribution, which provides PGAS mechanisms and language constructs such as remote procedure calls, virtual shared memory, and distributed objects. Saddlebags provides a programmer with objects and functions that are inherently distributed, and can be extended with methods and data to express different algorithms. This section will sparsely reference UPC++ terminology, and rather refer to general PGAS concepts, with the goal of presenting a design that is agnostic to communication libraries and implementation details.

\section{Motivation}
%pregel states that the synchronisity of iterations makes programs easier to reason about.
Saddlebags is designed with several key features in mind, all of them related to what responsibilities a parallel computing framework can take off the an application developer's hands. Insight from Pregel shows that a synchronous, iterative workflow makes distributed programs easier to reason about \cite{pregel}. Spark shows that capturing flexible applications with a single data structure and API makes development easier \cite{sparkarticle}. At the same time, it has been shown that minimizing data movement is of increasing importance in high performance applications \cite{abstractionslocality}, and that developing data  abstractions is a key solution for expressing parallel algorithms that handle an increasing amount of data \cite{hall2013rethinking}.

Some of the underlying concepts in Saddlebags and the motivation behind them follows:

\hspace{4ex} \textbf{1. Computational tasks should be executed in the same partition in which the data it requires is located}

%TODO: CITATION FOR DATA PARALLELISM
Problems that can be expressed with data parallelism \cite{data_parallelism} can be distributed across computing units as individual tasks related to specific data. A key concept within PGAS programming is that a private memory segment has affinity to a single computing unit. To exploit data locality, a computational task should only be performed on the unit of computing which the data it requires has affinity to. Expressing behavior like this is trivial when the distribution of data is deterministic. Another wording of the same behavior is that computational tasks have affinity to the data it requires, the same way data has affinity to a computing unit and its private memory segment.
\\

\hspace{4ex} \textbf{2. A multitude of data should be implicitly distributed across available partitions}

Following the principle of a computational task's affinity to its data, full utilization of available processing units is only possible when a program's data is distributed. This means that data used in parallel processing should always be distributed across available memory partitions, so that the computational tasks that require the data is equally distributed.
\\	
\newpage
\hspace{4ex} \textbf{3. The programmer should not need to write communication routines such as messages or remote procedure calls}

The purpose of building a framework on top of UPC++ is to provide interfaces for the implementation of distributed applications without the complexity of communication routines. While computation methods are application-specific, communication patterns can be implemented for a general case. When claiming that programmers are relieved of writing communication routines, it refers to the need for knowing where data is physically stored, which partition to communicate with, and the need for ensuring that messages are delivered before continuing computation. Data movement will still be explicit, but expressed in an abstraction level in which data locality and memory partitioning is not considered. Saddlebags presents a programming model that is devised from a specific communication model, and it is assumed that algorithms that can be expressed within this model can also be implemented to communicate in a general pattern. Predictable communication patterns also allows for optimizations and reduces the need for application-specific synchronization. This also makes the physical distribution of data transparent to the programmer.
\\

The goal of building a a framework on these specific features is to allow computational parallelism to be expressed with the same semantics as data parallelism, while still allowing tasks to communicate data between each other. The complexity of implementing applications that fit this classification is reduced by utilizing a general communication pattern that relieves programmers of implementing specialized communication. Programmers will in this framework use communication routines by referring to data identifiers rather than physical location.

Like several of the systems mentioned in section \ref{section:relatedwork}, Saddlebags implements a Bulk Synchronous Parallel programming model. Like Pregel, Saddlebags distributes a generic and extendable computational task with every data item. Like Husky, Saddlebags provides a more flexible data movement model between items, and does not aim at supporting graph-oriented workloads exclusively.

\section{Programming model}
The programming model and data flow model of Saddlebags is based around coupling data and functionality together into an object. Objects that serve the purpose of executing computational tasks in Saddlebags are named \emph{Items}. Items contain pre-existing general communication methods, and the main effort of developing an application with Saddlebags is implementing the computational tasks within an item, and integrating communication and data-flow into that task.

Items of different types can be implemented in the same application, and stored in different \emph{tables}. The behavior of tables cannot be extended by the programmer, but every application must define their tables by name and item type. Tables are common for every partition, while an instance and name of an item can only exist on a single partition. This is how distribution of data is achieved: items are spread across partitions, while the tables that contain them exist on every partition, and can reference items regardless of physical location.

\begin{figure}[h]
\centering
\includegraphics[width=14cm]{illustrations/basicmodel.pdf}
\caption{Overview of the organization of data structures in a Saddlebags application.}
\label{fig:basicmodel}
\end{figure}

The relation between items, tables and partitions are illustrated in figure \ref{fig:basicmodel}. This illustration shows a basic design of a Saddlebags application with two tables and two partitions. Underlined names are objects that exist on every partition. Lines indicate example communication, with red lines indicating communication between different partitions. Items of different types are stored in different tables, and the same tables are available on both partitions. Item names are unique across all partitions, and a single item have affinity to a single partition. 

Tables are stored within a \emph{worker}, a distributed object. Tables are referenced via this object. The behavior of the worker need not be extended by the programmer, and in the context of data flow and execution model the worker is relatively insignificant, and will be mostly ignored in this section. It will be explained further when discussing implementation and interoperability with UPC++.

Items implement a \emph{work-push-pull} model, in which \emph{work} refers to a computational task implemented in the item class, and \emph{push} and \emph{pull} are the two available modes of communication. The work of an item is designed to be executed iteratively, and each iteration of a Saddlebags application involves executing work and performing any communication routines that was called as a result of the work. An iteration applies to every item in every table, and it is with this iterative model that an application can be executed. 

\subsection{Events} \label{section:events}
The data flow model of Saddlebags is implemented with the concept of \emph{events}. Within the context of this framework, an event refers to the receiving of any communication message, or reaching specific points of the iteration process. Whenever an event is triggered, for example when receiving a \emph{pull} message, the item that received an event-trigger calls a corresponding method, in this case a \emph{foreign pull} method. The contents of this method can be implemented differently in different item types, and is also able to initiate other communication routines. This is directly related to the data flow of an application: when receiving a pull event, the event-triggered method determines what data to return. When the message returns to the original sender with some data, the event-triggered return method determines how to handle the data. 
\newpage

\begin{table}[h]
\setlength\arrayrulewidth{1pt}
\renewcommand{\arraystretch}{2}
\begin{tabular}{ | p{3cm} | p{6cm} | p{6cm} |}
   	\hline
   	\textbf{Method} & \textbf{Event} & \textbf{Implied data movement} \\ \hline
   	On Create & Item is created & None\\ \hline
  	Refresh & An item with the same name is attempted created & None \\ \hline
  	Work & Start of iteration & None \\ \hline
   	Foreign Push & A push message is received from another item & Data from sender is received with the message  \\ \hline
   	Foreign Pull & A pull message is received from another item & Return data is sent back to the sender\\ \hline
	Returning Pull & A pull message that originated in this item has returned & Data from pull target is received with the message\\ \hline
	Finishing Work & End of iteration & None\\ \hline

\end{tabular}
\caption{All events and their corresponding methods}
\label{table:events}
\end{table}
\vspace{1.5cm}

Table \ref{table:events} lists all methods in the item class that can be triggered by events. Column 1 lists the names of the methods, column 2 lists the circumstances under which the method is called, and column 3 lists what data is moved in the corresponding method.

All methods with data movement are called as a result of communication with other items. Communication routines can be initiated in any of the methods listed in the table. Items are referenced by name, so two items with the same name cannot exist in the same table. The \emph{Refresh} method is used to incorporate this behavior in a program.
\newpage
\subsection{Example: PageRank} \label{section:examplepagerank}
The programming model that is implied by items and events can be explained further through an example application. Consider the PageRank algorithm, a mathematical expression for ranking the importance of a website \cite{pagerank}. The following expression is a simplified way of calculating the Page Rank ($PR$) of a website ($u$), given a list of pages that link to it ($B_u$), and the total number of links from those websites ($N$):

\vspace{10pt}
\begin{center}
${\displaystyle PR(u)=\sum_{v\in B_{u}}{\frac{PR(v)}{N(v)}}}$

\end{center}
\vspace{20pt}

It is assumed that every page starts with a PageRank of 1. Given a list of pages and their outgoing links, a PageRank application can be designed with the Saddlebags programming model. Each page is implemented as an item, and each item is given a list of its own outgoing pages, in addition to a PageRank value and a buffer for calculating a new PageRank, set to 0. Only one table and one type of item is needed. The required methods are listed in table \ref{table:pagerankevents}. When the items are created, the algorithm can be ran for as many iterations of PageRank as needed, since one PageRank iteration (one execution of the aforelisted formula for each item) is equivalent to one Saddlebags iteration.

\vspace{1.5cm}

\begin{table}[h]
	\setlength\arrayrulewidth{1pt}
	\renewcommand{\arraystretch}{2}
	\begin{tabular}{ | p{3cm} | p{12cm} |}
		\hline
		\textbf{Method} & \textbf{Functionality} \\ \hline
		Work & Each page that this item links to will receive a push message with this item's PageRank divided by this item's count of outgoing links \\ \hline
		Foreign Push & Add the incoming value to the new PageRank buffer \\ \hline
		Finishing Work & PageRank of this item is set to the value of the buffer. The buffer is set to 0.\\ \hline
		
	\end{tabular}
	\caption{The methods required to implement PageRank with Saddlebags}
	\label{table:pagerankevents}
\end{table}

\section{Architecture and data flow}
\subsection{Table structure}
Programming a Saddlebags application involves two main tasks: extending item methods to fit a computational purpose, and creating items through data input. In the example illustrated in the previous section, table \ref{table:pagerankevents} shows the extension of an item, and feeding outgoing links to an item is an example of data input. Items are referred to by the name of the table that contains it, and the name of the item itself This is the only way to reference an item in Saddlebags, as tables are responsible for both item initialization and storage.

A table represents an associative array, a collection of unique keys, and values, in which keys are item names and values are item object instances.  Tables are initialized with a name and an item type, and each table can only contain items of a single type. There can be an arbitrary number of tables in a program, and item types are not tied to a single table instance. It is guaranteed that one instance of all tables are available on all partitions.




\subsection{Data distribution}
Each table is initialized with a distributor type, a specific class with a distribution method that determines the partition items should be located in. Since all data is stored in items, and all items are stored in tables, the distribution type of tables is the only component of a Saddlebags application that manages distribution of data and computation.

The distribution method that is common for all distributor classes takes an item name as parameter, and returns a partition identifier. By default, distribution is based on the hash digest of the item name, utilizing consistent hashing where items will be assumingly evenly spread around all partitions \cite{consistenthashing}. The motivation behind this is to distribute computational tasks as evenly as possible.

It is possible for different tables within the same program to use different distribution logic, since distributors are tied to tables. Figure \ref{fig:communication} shows an example application structure with two tables and two partitions. Table 1 in partition 0 sends a push message to an item in table 2, and the distributor in the instance of table 2 in partition 0 directs the message to partition 1. The initial message is only directed at a table and item, since this is the only information about the data layout that a programmer has access to. All tables are initialized on all partitions, so any table's distributor is available to all items. This enables communication between all tables and partitions.
\vspace{0.4cm}
\begin{figure}[H]
	\centering
	\includegraphics[width=13cm]{illustrations/communication.pdf}
	\caption{Example of communication between tables on different partitions.}
	\label{fig:communication}
\end{figure}

\subsection{Table locality}
When a table is created, it is specified whether it is a local or global table. Global tables utilize distributors, that can potentially distribute data across all partitions that are available in the system. Local tables does not utilize any distribution logic, and all items are stored within the same partition as they were created. While local tables are created on every partition, their items cannot be remotely accessed. The items in global tables can always be remotely accessed, regardless of their distribution~model.

Note that in figure \ref{fig:communication}, table 1 is classified as local, while table 2 is classified as global. Table 1 has the same items on both partitions, while table 2 has different items. If the same items were added to table 2 from both partitions, they would still only be stored in one partition, as the distributor would redirect the item creation. From a programming standpoint, communication with local or global tables looks identical, but setting the locality of a table can have an impact from a data movement and optimization standpoint, since communication with a local table will never implicitly invoke remote operations.


%The communication patterns of a push/pull paradigm is simple enough that the cost of data movement as a result of communication in an application can be reasoned about, despite implementation complexity being hidden. 

%Saddlebags is designed with the goal that abstractions provided by the programming model, through C++ mechanisms such as inheritance and type templates, should not affect algorithmic complexity. This will likely only be true for algorithms that can be tailored to the push/pull/work paradigm. 

\section{Fault tolerance}
Saddlebags takes an approach to achieving a degree of fault tolerance by utilizing replication. Each partition will act as a replica for one or more partitions with subsequent rank values, looping back to partition 0 at the highest rank. The degree of fault tolerance, i.e. the number of replicas per node, is defined on a per-application basis. This is similar to the hash ring approach found in the key/value storage system Dynamo \cite{dynamo}.

Replicas are updated at the end of each iteration, which means that every node is in a state where no updates occur when replication routines are executed. This makes for low communication overhead during replication, since all contents of a table can be sent in one message. This raises an issue with inconsistency, if a node crashes during computation. A situation like this must be solved with a rollback operation, moving every table to the previous iteration. This is achievable with the same replication routines, since a back-up of the state of the previous iteration exists in the current replicas. This means that, to rely on rollbacks, a replication degree of at least 2 must be used. 

Figure \ref{fig:ring} shows an overview of 8 nodes and their replicas, when a program is initiated with a fault tolerance degree of 2. Each letter A-H represents a node, partition, and part of the key space of the hash digests of item names.

\vspace{1.5cm}
\begin{figure}[H]
	\centering
	\includegraphics[width=15cm]{illustrations/ring.pdf}
	\caption{Hash ring-view of Saddlebags application with 8 nodes and a fault tolerance degree of 2}
	\label{fig:ring}
\end{figure}

\newpage
\mbox{}
\newpage
\chapter{Implementation}
This chapter will present the implementation of Saddlebags, explaining how the design translates into different data structures, and how programs can be written within this framework. Saddlebags was implemented in the C++ programming language, and it can be used to write parallel programs in C++ by importing its header file. Saddlebags is built on the UPC++ library for C++, and this library is a requirement in order to compile Saddlebags programs. Because of this dependency, Saddlebags poses the same system requirements as UPC++. In brief, it can be used on Linux with $gcc$, on macOS version 10.11 or newer, and on Cray XC supercomputers \cite{upcreq}. On Linux clusters with MPI support, UPC++ programs can be executed with MPI commands~\cite{upcmpi}.

\section{Overview}
The previous chapter presented an object-oriented approach to parallel computation, in which problems were split up into separate items that could be distributed and accessed independently of other items. Defining computational tasks in items, and passing data as messages between items with different tasks, is the main effort of implementing programs in Saddlebags. The final program structure of the framework was devised in the development of an interface in which items of different types were interoperable, through a common messaging interface.

\texttt{Item} is implemented as a C++ class, consisting of pre-defined communication methods, in addition to several empty methods, corresponding to the events listed in section \ref{section:events}. To develop meaningful programs, some of these events must be extended. This is done through inheritance. Event-methods are marked as \texttt{virtual}, so that they can be overridden. Non-event methods, such as push and pull, implement generic functionality that will remain the same for all item classes. This leaves the \texttt{virtual} methods to indicate the interface in which events can be implemented. 

Items are stored in hash maps, which are defined in a \texttt{TableContainer} class. The hash map and table container makes up the functionality of the Table described in chapter~\ref{chapter:design}. \texttt{TableContainer} inherits from a \texttt{TableContainerBase} class that contains functionality that is applicable for all item types. The reason for this will be expanded on in section \ref{section:templates}. Table containers have default logic for distributing items across partitions, and this can be exchanged for specialized solutions by passing a \texttt{Distributor} object. The \texttt{Distributor} class implements a \texttt{Distribute()} method that takes a item key as parameter, and returns an integer corresponding to the rank in which that item should be stored. 

An application can consist of multiple tables, containing different item types. Tables are stored in the \texttt{Worker} class, which is responsible for synchronization, initiating computational tasks through the events \emph{Work} and \emph{Finishing work}, and providing a common interface for different items to access each other in order to deliver messages. Every table and item in a program can be referenced through its worker. A Saddlebags program is initiated by creating a worker, and there is little purpose in having multiple simultaneous workers.


%message class

The main classes in the Saddlebags implementation is shown as a UML diagram in figure \ref{fig:uml}. In practice, an application will have at least one additional class, inheriting from the \texttt{Item} class. 



\begin{figure}[H]
	\centering
	\includegraphics[width=15cm]{illustrations/uml.pdf}
	\caption{UML diagram of Saddlebags' main classes.}
	\label{fig:uml}
\end{figure}


\section{Template types} \label{section:templates}
Template programming is a feature of C++ that enables the use of generic types when defining classes and functions. A generic name can be used in place of a specific data type in the definition of a class or function, enabling more flexible declarations and reducing code duplication. When an object is initialized, generic types must provided, in addition to the regular constructor parameters. Template programming was used heavily in the implementation of UPC++ \cite{upc_language_specification}. Listing \ref{lst:template} shows an example of the syntax of declaring a template class with one template typed property, and declaration of a child class.

Template programming is used in the implementation of Saddlebags for two reasons. Firstly, to achieve a general-purpose programming model, a framework user should be able choose the data types that suits a particular problem. Secondly, the item extension model described in chapter \ref{chapter:design} requires the framework to be able handle different item classes.
\\ \\
\begin{lstlisting}[label={lst:template}, float=h,frame=tlrb, caption={Declaration and initialization of an inherited template class in C++}, captionpos=t, language=c++, showstringspaces=false]

template<class T>
class MyClass
{
  T value;
};

template<class T>
class MyChildClass : public MyClass<T>
{
  T otherValue;
};

auto obj = MyChildClass<int>();
obj.value = 34; //obj.value is of integer type
\end{lstlisting}
\newpage
\subsection{Parameters}

Every class listed in the diagram in figure \ref{fig:uml} is a template class. The \texttt{Worker}, \texttt{TableContainerBase}, \texttt{TableContainer}, \texttt{Item}, and \texttt{Message} classes take at least the three following template parameters.
\\

\hspace{4ex} \textbf{\texttt{TableKey\_T}}
\\
The type of the variables that tables are referenced by. Every table has a name that is stored with the same type, defined by this template parameter. The parameter is needed in the Worker because references to the tables are stored in this class. It is needed by Item classes because communication methods require a table name to reference other items to communicate with. It is needed in the TableContainer class because the table is responsible for initializing items. It is needed by the message class because the destination table and object must be stored in outgoing messages.
\\
 
\hspace{4ex} \textbf{\texttt{ItemKey\_T}}
\\
The type of the variables that items are referenced with in a table. A value of this type is used whenever an item is accessed. A worker and table can access items by looking up its key, and items can communicate with other items by initializing messages that contain item keys. Furthermore, items can only be initialized by passing an item key to the table.
\\

\hspace{4ex} \textbf{\texttt{Message\_T}}
\\
The type of the data that is communicated through messages, i.e. push and pull operations. The Message class itself has several fields, all of which are listed in the diagram in figure \ref{fig:uml}. However, the payload of this message is a single field of \texttt{Message\_T} type, which is delivered to push and pull methods. Messages are delivered to items between iterations, so other classes also require this template parameter because messages can be buffered on delivery. Optional methods for communicating data is explained in the next section, but this type determines the data type to send~regardless.
  
The \texttt{TableContainer} class takes an additional template parameter that is the Item type stored in that table. Since multiple tables can have different item types, the item type is given only to the table. The Item type of each table is transparent to the Worker, so that it can store an arbitrary number of tables in its \emph{tables} hash map. This is achieved through referencing \texttt{TableContainer} instances as \texttt{TableContainerBase} types - a parent class that does not take an Item type template parameter. This is possible because the only methods from an Item that is relevant for a Worker is the methods that are available in the parent Item class. Tables are stored in a Worker as values in an \texttt{unordered\_map} from the C++ standard library.

In practice, this mean that the Worker contains a map with the key type 
\texttt{TableKey\_T} and value type \texttt{TableContainerBase<TableKey\_T, ItemKey\_T, Message\_T>*}. This allows the Worker to store all tables, in actuality of different types derived from\\ \texttt{TableContainerBase}, within the same data structure. When a table is created, the pointer that is passed to the Worker is of type \texttt{TableContainer<TableKey\_T, ItemKey\_T, Message\_T, ItemType<TableKey\_T, ItemKey\_T, Message\_T> >*}\\ replacing \texttt{ItemType} with a relevant child of the Item class.

Similarly, a table can be given a \texttt{DistributorType} as a template parameter to customize the distribution logic. This is also not present in the \texttt{TableContainerBase} class and is transparent to the Worker.

From this design, it seems possible to move the transparency of Item types from the Worker to the Table, so that there is only one \texttt{TableContainer} class; the same logic that allows a Worker to access tables from a common parent class also applies to accessing items from their parent class, since there is already need for at least one Item child class. However, a parameter with the child item class is needed in the Table because this is where Items are initialized. The \texttt{TableContainer} class is responsible for initializing items because item keys are unique, and attempts to initialize an item with a pre-existing key will result in calls to the Refresh method rather than actual initialization, as shown in table \ref{table:events}.


% for two reasons: item types and generic framework
% templates in each class: tablekey itemkey messagetype
% table defined with inherited item class
% item class in table is transparent to worker, worker has basetablecontainer
% this allows aribtrary number of tables with any item class
% <listing of declaration of worker and table from pagerank (with strings as item key)>

% table could be used with base item type and no template since api is the same, but child item class is needed for initialization within table, and initialization in table is necessary since table ID is the only way to reference item

\section{Communication}
Partitions are executed in parallel, and the thread spawning and memory partitioning is handled by UPC++. Communication between partitions is achieved through UPC++ Remote Procedure Calls, in which a function is passed from one partition to another, where it is executed. As parameters, RPCs can use variables passed as value, or references to distributed objects from UPC++. In Saddlebags, partitions need to perform RPCs to initialize remote items, or perform remote pushes and pulls. For this, it is necessary for items to communicate with other items, even those in different tables. In order to achieve a design where every table can reference every other table, the worker is declared as the only distributed object in a Saddlebags program. The UPC++ Distributed Object class is a template class with a parameter to indicate the type of the target object. In this case, the distributed worker that can be used in RPCs is of type \texttt{upcxx::dist\_object<Worker<TableKey\_T, ItemKey\_T, Message\_T> >}. The underlying Worker object can be accessed using the dereferencing operator \texttt{*}.

A worker is initialized as a distributed object synchronously across all partitions, which is recommended by the UPC++ specification \cite{upc_language_specification}. Additionally, tables are created synchronously, to match the transparent distribution of items. With tables having a reference to the distributed worker, all items and tables can be referenced via RPC.

\subsection{Sending modes} \label{section:sendingmodes}
Communication between items based on push and pull methods. Saddlebags implements several different techniques for handling these communication routines, which is determined by a \texttt{sending\_mode} property in the worker. The following modes are implemented.
\newpage
\hspace{4ex} \textbf{\texttt{Buffering}}
\\
Push and pull operations are stored as messages, in which the table and item name of both the sender and receiver is stored. Messages are sent when the communication method is called, but received messages are buffered and not applied until after the work-phase is finished. The purpose of the buffering is to enforce a bulk synchronous parallel execution model.\\

\hspace{4ex} \textbf{\texttt{Combining}}
\\
Similar to the Buffering mode, but messages are not sent until the work-phase is completed, so that all messages with the same source and destination partitions are sent in a single RPC. This is implemented on the sender-side by buffering messages in an associative data structure with destination partition as key, and a vector of messages to that partition as value.\\

\hspace{4ex} \textbf{\texttt{Direct}}
\\
Operations are not stored intermediately as messages, but rather only expressed through the execution of its RPC. Does not enforce any synchronization, so push and pull operations can be executed both before or after work. \\

\hspace{4ex} \textbf{\texttt{GasnetBuffering}}
\\
Like the Direct mode, operations are only represented as an RPC. Like the Buffering mode, operations are performed in a bulk synchronous parallel pattern. This is achieved by not advancing UPC++ state, by not calling the \texttt{progress} function until all partitions has completed the work phase. This means that any buffering of operations is handled by the internal communication routines of GASNet. Other buffering techniques are preferred because GASNet cannot store more than 65536 outstanding operations \cite{gasnetpresentation}. This mode was implemented for the experimental purpose of removing the overhead of the Message abstraction.


\subsection{Ordering}
Messages are by default not received in a deterministic order. A configuration parameter can be set to execute returning pull-messages in the same order as they were sent out. Pushes does not have a defined desired order, since they, from the receiver's perspective, are sent in parallel from multiple items acting independently. Pulls has a defined desired order in which they are returned, and that order is the same that as they were sent out in. 

Ordered pulls functionality is implemented by each item creating a buffer for returning pulls equal to the size of outgoing pulls, and attaching a sequence number to every message before sending them. When a pull returns, the message is stored at the location in the buffer equal to its sequence number. This results in an ordered list of pulls. This approach is possible because the total number of outgoing messages is known at a point before any messages return, so that the buffer can be created with the desired size ahead of time.

It is not possible to order pulls using the Direct sending mode, because in this mode, some messages can be returned before all messages have been sent. It is also not possible to order pulls with the GasnetBuffering sending mode, since the ordering techniques requires using the message data structure. 

%rpc, distributed object
%caching futures
%message
%sending modes!

\section{Table}
Items are stored in a hash table, which is implemented with a Robin Hood Hashing model~\cite{robinhash}. This hash table design is based around open addressing, meaning that all entries are stored in a single contiguous array, and linear probing, meaning that hash table collisions during insertions are handled by attempting to perform the insertion in the next entry in the array instead. When performing linear probing, a Robin Hood table will swap entries if the item that is currently being inserted is further from its desired position than the item in the entry that the probe is inspecting.
\newpage
For the correctness of the program, the necessary factors are the hash maps semantics: one-to-one association between keys and item instances, and the functionality of iterating through the map, accessing each item once. Further implementation details about the hash table are optimization decisions. Robin Hood hashing was chosen because it demonstrates a lower worst-case lookup time in comparison to other common designs, including the unordered map from the C++ standard library  \cite{hashbenchmark}. A custom hash table implementation was used so that parameters such as maximum load factor and initial table size could be tweaked for optimization.

The item type of a specific table is transparent to a Worker, which is the object that a programmer will primarily interact with. A function is provided that returns a C++ iterator to the hash table in which entries, which are pointers to a child of the Item class, are reinterpreted to the actual child item type. This allows a programmer to use C++ iterators to lookup all items stored in a partition, without only using references to objects of the parent item type, which is what is provided to workers.

\section{Synchronization}
The only synchronization mechanisms adopted in Saddlebags are barriers and futures. UPC++ programs consist of a fixed number of threads, in which only one thread has access to each private memory segment. Multiple threads per partition can be used with libraries such as OpenMP \cite{openmp}, but this technique is not used in Saddlebags. This means that sequential data structures can be used internally in the framework, since each partition contain their own instances of objects. For example, the hash table implementation does not contain any synchronization mechanisms or other parallel features. Pitfalls like hash table expansions during insertions, or insertions to the same hash table entry, are resolved by only calling UPC++ Progress in-between atomic operations. It is guaranteed that entire RPCs will be completed before yielding control back to the main execution of a partition. Calling progress to resolve outstanding operations is handled internally and is not the responsibility of a Saddlebags user.


% no synch for expansion etc due to progress
% one way communication futures are stored
% barriers in cycle
%
%\section{Usage}

\newpage
\chapter{Evaluation}
This chapter will provide an overview of the features and performance of Saddlebags in comparison with other systems. Section \ref{section:programmability} focuses on programmability features, providing an overview of the features that are available when expressing algorithms in bulk synchronous parallel frameworks, comparing Saddlebags, Husky and Pregel.

Section \ref{section:performance} will provide experimental performance comparisons with other systems, through PageRank and Term Frequency-Inverse Document Frequency benchmarks in Saddlebags, Spark and Husky.

Section \ref{section:configuration} provides experimental performance comparisons between different configurations of a Saddlebags application, providing an overview of the expected performance penalties of sending modes and message ordering.

\newpage
\section{Programmability}\label{section:programmability}
This section will overview of the programmability of Saddlebags in comparison with other systems with similar data distribution and execution models, namely Pregel and Husky. Both these frameworks provide examples of implementations of the PageRank algorithm in their papers \cite{husky} \cite{pregel} to demonstrate programming models, and therefore this application is used as a baseline for a comparison of programmability. The PageRank algorithm was explained more in-detail in section \ref{section:examplepagerank}. Listing \ref{lst:pagerankhusky} contains C++ code for definition of a PageRank vertex in Husky, taken from an official repository of Husky example programs \cite{huskyexample}. Listing \ref{lst:pageranksaddle} contains C++ code for a PageRank item class in Saddlebags. Listing \ref{lst:pagerankpregel} contains C++ code for a PageRank vertex class in Pregel, taken from the Pregel paper. 

\begin{lstlisting}[label={lst:pagerankhusky}, float=h!,frame=tlrb,numbers=left, caption={PageRank vertex class in Husky}, captionpos=t, language=c++, showstringspaces=false]

class Vertex {
  public:
  using KeyT = int;
  Vertex() : pr(0.15) {}
  explicit Vertex(const KeyT& id) : vertexId(id), pr(0.15) {}
  const KeyT& id() const {
    return vertexId;
  }
  friend husky::BinStream& operator<<(
         husky::BinStream& stream,const Vertex& u) {
    stream << u.vertexId << u.adj << u.pr;
    return stream;
  }
  friend husky::BinStream& operator>>(
         husky::BinStream& stream, Vertex& u) {
    stream >> u.vertexId >> u.adj >> u.pr;
    return stream;
  }
  int vertexId;
  std::vector<int> adj;
  float pr;
};
\end{lstlisting}

\begin{lstlisting}[label={lst:pageranksaddle}, float=h!,frame=tlrb,numbers=left, caption={PageRank item class in Saddlebags}, captionpos=t, language=c++, showstringspaces=false]

class SiteItem : public saddlebags::Item<int, int, float> {
  public:
  float page_rank = 1;
  float new_page_rank = 0;
  std::vector<int> links;
  void do_work() override {
    for(auto it : this->links) {
      this->push(0, it, page_rank/((float)links.size()));
    }
  }
  void foreign_push(float val) override {
    new_page_rank += val;
  }
  void finishing_work() override {
    page_rank = new_page_rank;
    new_page_rank = 0;
  }
};
\end{lstlisting}

\begin{lstlisting}[label={lst:pagerankpregel}, float=h!,frame=tlrb, numbers=left,caption={PageRank vertex class in Pregel}, captionpos=t, language=c++, showstringspaces=false]

class PageRankVertex : public Vertex<double, void, double> {
  public:
  virtual void Compute(MessageIterator* msgs) {
    if (superstep() >= 1) {
      double sum = 0;
      for (; !msgs->Done(); msgs->Next())
        sum += msgs->Value();
      *MutableValue() = sum;
    }
    if (superstep() < 30) {
      const int64 n = GetOutEdgeIterator().size();
      SendMessageToAllNeighbors(GetValue() / n);
    } else {
      VoteToHalt();
    }
  }
};

\end{lstlisting}
\newpage
In terms of code length, all three frameworks are similar. Husky shows the longest class definition, at 23 lines of code, and Pregel shows the shortest, with 18 lines of code. This can likely be considered insignificant. A large portion of the Husky class is used to define communication routines with stream objects. Saddlebags and Pregel avoids having to explicitly define specialized communication routines, by using a template parameter to define a specific messaging type. In this example, Saddlebags uses floats, while Pregel uses doubles.

Both Saddlebags and Pregel define the computational task within the class. The Pregel class defines a single method, Compute, that contains all application-specific logic. Saddlebags splits this into several methods: work before communication, methods to call as a result of communication, and work after communication. The single compute method in Pregel allows iteration over all incoming messages, a feature that in Saddlebags is available through the \texttt{foreign\_push} and \texttt{foreign\_pull} methods. These methods are equivalent to message iterator code blocks in Pregel.

Messages that are delivered in the Compute method in Pregel is equivalent to push-messages in Saddlebags, and it is in both frameworks implemented in the receiver how to handle these messages. Saddlebags' pull messages and \texttt{foreign\_pull} method does not have an equivalent implementation in Pregel. Instead, each vertex has a defined \texttt{MutableValue} that can be fetched by other vertices, and it is assumed that a vertex will always transfer the same data item. In Saddlebags, what value is fetched is defined by the receiver. The Saddlebags approach offers more flexibility, at the cost of implementation complexity. In Saddlebags, all data properties in an Item must be defined per-application, and there are no implicit assumptions about what data to return on incoming pulls, save for the template parameter \texttt{Message\_T}. Both Saddlebags and Husky explicitly defines a variable \texttt{pr} or \texttt{page\_rank} for storing relevant data, while in Pregel, only the \texttt{MutableValue} property is used.

The number of iterations of the algorithm to perform is expressed in the Pregel class through the \texttt{supersteps} function. Neither Saddlebags nor Husky has this expressed, and must rely on other parts of the program to initiate iterations and manage termination. A larger portion of the runtime of the algorithm is expressed in Pregel vertices than in Husky vertices and Saddlebags items, as it is able to define data, computational task, and termination, with some implicit communication routines. Saddlebags items are able to define data, computational tasks, and communication routines. Husky vertices can express data, and communication routines. Notice that in this example, the mathematical PageRank algorithm and initiation of messaging is not implemented in the Husky class, and is deferred to the main function of the program, after vertices has been created. However, this differentiation of computation code is only semantic, as all three frameworks distributes execution of tasks to the same computing units as where the relevant data items are stored.

Expressiveness features are difficult to quantify, and is in this comparison presented through observation of code examples and design overviews. The findings are shown in table \ref{table:programmability}. "Per application" refers to features that can be expressed, but are not present by default.


\begin{table}[b!]
	\begin{center}

	\setlength\arrayrulewidth{1pt}
	\renewcommand{\arraystretch}{2}
	\begin{tabular}{ | p{4cm} | p{3.5cm} | p{3.5cm} | p{3.5cm} |}
		\hline
		\textbf{Functionality} & \textbf{Saddlebags} & \textbf{Pregel} & \textbf{Husky}\\ \hline
		\textbf{Implicit distribution of data} & Yes & Yes & Yes \\ \hline
		\textbf{Message types} & push/pull & push/pull & push/pull \\ \hline
		\textbf{Payload of messages} &Return values of methods  & One common data element & Per-application definition of in/out streams\\ \hline
		\textbf{Ordering of messages} & Available, for pulls & No & No \\ \hline
		\textbf{Computation code} & In item, in multiple stages, in methods & In vertex, in one stage, in method & Defined in main program, iterating over vertices \\ \hline
		\textbf{Terminate on convergence} & No & Yes (explicit in task) & No \\ \hline
		\textbf{Adjacent vertices} & Per-application & Yes & Per-application \\ \hline
	\end{tabular}
	\caption{Features of item/vertex objects in Saddlebags, Pregel and Husky}
	\label{table:programmability}

	\end{center}
\end{table}

\section{Performance evaluation} \label{section:performance}
Benchmarks were performed on three different systems:

The first system, a desktop machine, has an Intel Core i5-4460 CPU @ 3.20 GHz, with 4 cores and 256 KiB, 1 MiB and 6 MiB L1, L2, and L3 caches respectively. This system has 8 GB memory and runs Ubuntu 16.04.

The second system, a university provided server, has an Intel Xeon E5-2650L v3 CPU @ 1.80GHz, with 24 cores and 768 KiB, 3 MiB and 30MiB L1, L2, and L3 caches respectively. This system has 64 GiB memory and runs CentOS 7.5.1804.

The third system is the Abel computing cluster at the University of Oslo. Abel consists of 650+ compute nodes, with Intel E5-2670 CPUs @ 2.6 GHz, with 16 cores and 64 GiB memory. At this time, Abel uses CentOS 6 \cite{abel}. Nodes are connected with FDR InfiniBand. In this evaluation, only one core is used per node. This is the default behavior of the workload manager available on this cluster \cite{abeluserguide}, and the recommended way of running Spark jobs \cite{abelspark}. 

This selection of systems is used because it allows for both single node and distributed workloads. The 4-core Ubuntu system is used because of compatibility issues with Husky and its dependencies and CentOS. The 24-core server is used so that single node benchmarks can be ran with more than 4 partitions.

PageRank and TF-IDF are used as benchmarks. This selection is discussed further in chapter \ref{section:discussion}. PageRank benchmarks were performed on Kronecker graphs. Kronecker graphs were modeled after properties from real networks, with the purpose of providing a scalable generative mathematical graph model \cite{leskovec2010kronecker}. For this evaluation, Kronecker graph generation was performed with the BigDataBench benchmark suite \cite{wang2014bigdatabench}. This Kronecker graph generator accepts an input of the number of iterations to perform the generation algorithm. Graphs nodes are represented as 32 bit integers. The iteration count, in addition to the total size of the graph, is given in each individual benchmark result.

\newpage
\subsection{Husky} \label{section:huskycomp}
A comparison with Husky was ran on the desktop system. A PageRank example program, parts of which were shown in section \ref{section:programmability}, from the official Husky repository was used. Husky was launched with 4 workers, which is equivalent to 4 parallel threads.Saddlebags was launched with 4 partitions, and uses the default sending mode, which is \texttt{Buffering}.

Results are shown in figure \ref{fig:huskypagerank}. In this benchmark, Saddlebags completed the task in shorter time than Husky on three different Kronecker graphs. It is shown that both the base execution time, and the scaling with dataset size, is slower in Husky than in Saddlebags.

\vskip 2cm

\begin{figure}[H]
	\centering
	\includegraphics[width=15cm]{illustrations/huskypagerank.pdf}
	\caption{Execution time of PageRank in Saddlebags and Husky}
	\label{fig:huskypagerank}
\end{figure}



\newpage
\subsection{Spark}
Comparisons with Spark were ran on the Abel cluster. For the PageRank benchmark, both a native Spark RDD MapReduce implementation, and a GraphX implementation, was used. GraphX is an API for running graph-based parallel jobs in Spark \cite{graphx}. Both the MapReduce \cite{sparkpagerank} and GraphX \cite{graphxpagerank} implementations were provided by official repositories. It is in the Spark implementation recommended to utilize GraphX to process this particular workload.

The other benchmark used with Spark is Term Frequency-Inverse Document Frequency (TF-IDF) \cite{tfidf}. This is an algorithm for classifying the relevance of words within a set of documents. A given word's importance within a given document is ranked based on its frequency within that document, and the inverse of the frequency of which it occurs in other documents.

Results are shown in figures \ref{fig:pagerankspark} and \ref{fig:tfidfspark}. Both experiments show results that are significantly in favor of Saddlebags. These results are consistent with those presented in the Husky paper \cite{husky}, when using the comparison between Saddlebags and Husky from section \ref{section:huskycomp} as baseline. The advantage over Spark is explained by Husky as the result of fine-grained access of mutable data, which is also a relevant factor for Saddlebags, and applies to both benchmarks.

It can be inefficient to express communication through a network graph with only coarse grained parallel operations, as nodes can communicate with remote nodes regardless of the distribution of data. Similar issues arises with the TF-IDF algorithm, in which a distribution of documents across compute nodes is intuitive. Since the total occurrences of words across all documents is a relevant data item when calculating the final TF-IDF value, communication across compute nodes is unavoidable with this algorithm. The difficulty with these algorithms illustrates the merits of fine grained remote data access.

\newpage
\begin{figure}[H]
	\centering
	\includegraphics[width=15cm]{illustrations/sparkpagerank.pdf}
	\caption{Execution time of PageRank in Saddlebags and Spark}
	\label{fig:pagerankspark}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=15cm]{illustrations/sparktfidf.pdf}
	\caption{Execution time of TF-IDF in Saddlebags and Spark}
	\label{fig:tfidfspark}
\end{figure}
\newpage
\subsection{Replication}
asd

\newpage
\begin{figure}[H]
	\centering
	\includegraphics[width=15cm]{illustrations/huskyfaulttolerance.pdf}
	\caption{Sending modes cluster}
	\label{fig:huskyfaulttolerance}
\end{figure}

\newpage
\section{Internal configuration} \label{section:configuration}
\subsection{Sending modes}
Different sending modes were outlined in section \ref{section:sendingmodes}. The PageRank benchmark was ran with different sending modes to investigate their performance implications. This benchmark was performed on the 16 core server and the cluster with 16 nodes. Both systems were included because of how UPC++ and GASNet communication implementation differs from single nodes and distributed systems. On the server, communication is achieved through shared memory. On the cluster, communication is achieved through message passing over InfiniBand.

Figure \ref{fig:sendingmodes} shows results from benchmarks on the server. In this environment, \\\texttt{GasnetBuffering} showed the worst performance. This sending mode avoids the Message abstraction, and achieves bulk synchronous parallel behavior by ignoring incoming RPCs until all local work has been completed. \texttt{Direct}, which also avoids the Message abstraction but handled RPCs immediately and disables bulk synchronous parallel behavior, showed the best performance, similar to \texttt{BufferingWorker}. Figure \ref{fig:sendingmodesabel} shows results from benchmarks on the cluster. Here, \texttt{Combining} showed the best performance by a significant margin, followed by \texttt{GasnetBuffering}. \texttt{Buffering} and \texttt{Direct} showed similar performance.

It is clear from these results that combining messages to reduce communication overhead is an effective technique in cluster environments. On single nodes, where communication is not executed through message passing, combining seems to have little effect, and may be unbeneficial because of the complexity introduced by the combination functionality.
It can also be observed that Buffering is less performant than avoiding the Message abstraction on the cluster. The poor performance of \texttt{GASNetBuffering} on the single node is also notable. It has been claimed that UPC++ programs will execute sub-optimally when few \texttt{progress}-calls are made \cite{gasneteventissues}, which is the case for this mode, due to caching issues. This behavior varying between single nodes and cluster environments would explain differences in observed results between the two.



\newpage
\begin{figure}[H]
	\centering
	\includegraphics[width=15cm]{illustrations/sendingmodes.pdf}
	\caption{Sending modes single node}
	\label{fig:sendingmodes}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=15cm]{illustrations/sendingmodesabel.pdf}
	\caption{Sending modes cluster}
	\label{fig:sendingmodesabel}
\end{figure}
\newpage
\subsection{Message ordering}


%\section{Energy efficiency}
%\section{Abstraction costs}
%\section{Comparison with other systems}

\newpage
\chapter{Discussion and Future Work} \label{section:discussion}

\section{Benchmarks}

\section{Fault tolerance}

\section{Message types}
%global pointers
%boost any

\section{Future work}
\subsection{View based serialization}
\subsection{File input}
%input of file formats?
%input of filled-out tables, like how rdds can be stored to disk and loaded

%other features from spark


\chapter{Conclusion}
lorem ipsum

\pagebreak
\bibliographystyle{ieeetr}
\bibliography{thesis}

\appendix
\chapter{Installing and using Saddlebags}
This section will explain how to install UPC++, and use Saddlebags on different platforms. All compute clusters with InfiniBand or Ethernet connections that support MPI is expected to work, but this section will focus on major Norwegian high performance computing infrastructure, namely the Abel cluster at University of Oslo, and the Stallo cluster, at the University of Tromsø. 

\section{UPC++ Installation}
Saddlebags has been implemented and tested with UPC++ v2018.1.0, which can be found at {\url{https://bitbucket.org/upcxx/upcxx/downloads/upcxx-2018.1.0.tar.gz}}. Recent releases can be found at the official UPC++ repository, at {\url{https://bitbucket.org/berkeleylab/upcxx/wiki/Home}}.
\subsection{Linux installation}
Unzip the tarball, and run the \texttt{install} file, with installation directory as argument:\\
\phantom{11111} \texttt{\$ ./install /desired/installation/directory}\\
Store the installation directory in an environment variable\\
\phantom{11111} \texttt{\$ export UPCXX\_INSTALL=/desired/installation/directory}\\
\subsection{Linux running}
asd
\subsection{Abel installation}
Abel uses the Lmod environment module system. Before installation can be performed as in the previous section, the proper modules must be loaded. \\
\phantom{11111} \texttt{\$ module purge}\\
\phantom{11111} \texttt{\$ module load python2/2.7.10.gnu}\\
\phantom{11111} \texttt{\$ module load gcc/7.2.0}\\
\phantom{11111} \texttt{\$ module load openmpi.gnu/2.1.0}  \\
First, any pre-loaded modules are removed to ensure that the intended compiler is used. MPI is needed for communication between nodes, and thread spawning. Python is needed for installation 

\phantom{11111} \texttt{\$ ./install /upcxx/installation} \\


\chapter{API Reference}
This section will explain the API that can be used to implement programs in Saddlebags. Since template types in definitions, parameters, and return values, can make function signatures exorbitantly large, simplified versions are shown at first, with parameters and return types explained subsequently. Because classes can't hold references to themselves as distributed objects, many operations are implemented as functions rather than methods.
\vskip 0.6cm
\begin{center}
	\noindent\rule{4cm}{1.2pt}
\end{center}
\vskip 0.5cm
\textit{function} \texttt{create\_worker()}\\
\phantom{11111} Initializes and returns a Worker with the given template parameters, and the \texttt{Buffering} sending mode. It is returned as a UPC++ distributed object. \\
\textbf{Template arguments:}\\
\phantom{11111} \texttt{<class TableKey\_T, class ItemKey\_T, class message\_T>}\\
\textbf{Return type:}\\
\phantom{11111}  \texttt{upcxx::dist\_object< Worker<key\_T, value\_T, message\_T> >}\\
\textbf{Variations:}\\
\phantom{11111} \texttt{create\_worker(SendingMode mode)}\\
\phantom{11111}\phantom{11111} Returns a Worker with the given sending mode.

\newpage
\textit{function} \texttt{add\_table(worker, tableKey, is\_global)}\\
\phantom{11111} Adds a table to a worker. Template parameters indicate the Item type this table should contain, and the data distribution behavior this table should adopt.\\
\textbf{Arguments:}\\
\phantom{11111} \texttt{worker} - The Worker that a table will be added to\\
\phantom{11111} \texttt{tableKey} - The key that the table is indexed with in the Worker. The type of this parameter is given by the Worker's \texttt{TableKey\_T}\\
\phantom{11111} \texttt{is\_global} - Boolean. Whether the new table should be globally accessible

\textbf{Template arguments:}\\
\phantom{11111} \texttt{<class DistributorType, class ItemType>}\\
\textbf{Return type:}\\
\phantom{11111}  void\\
\textbf{Variations:}\\
\phantom{11111} \texttt{DistributorType} template argument can be ignored, to initialize a table with default hashing distribution logic.

\vskip 0.05cm
\begin{center}
	\noindent\rule{4cm}{1.2pt}
\end{center}
\vskip 0.15cm
\textit{function} \texttt{insert\_object(worker, tableKey, itemKey)}\\
\phantom{11111} Attempts to insert an object into a given table. If the table does not contain an item with the given key, a new item is created. If an item exists, its \texttt{refresh} method is called.  \\
\textbf{Arguments:}\\
\phantom{11111} \texttt{worker} - The Worker that contains the table the item should be inserted into\\
\phantom{11111} \texttt{tableKey} - The table that the item should be inserted into\\
\phantom{11111} \texttt{itemKey} - The key to associate the new item with\\
\textbf{Return type:}\\
\phantom{11111}  void\\
\textbf{Variations:}\\
\phantom{11111} \texttt{<ItemType\_T> ItemType\_T insert\_and\_return(worker, tableKey, itemKey)}\\
\phantom{11111}\phantom{11111} Given the item type as template parameter, the item that is referred to is returned.
\newpage

\textit{function} \texttt{lookup\_item(worker, tableKey, itemKey)}\\
\phantom{11111} A pointer to an Item is returned if it exists with the given key, in the given table, in the given Worker. Otherwise, \texttt{NULL} is returned.\\
\textbf{Arguments:}\\
\phantom{11111} \texttt{worker} - The Worker that contains the table to look for the item in\\
\phantom{11111} \texttt{tableKey} - The table that contains the item to look for\\
\phantom{11111} \texttt{itemKey} - The desired item\\
\textbf{Template arguments:}\\
\phantom{11111} \texttt{<class ItemType\_T>}\\
\phantom{11111}\phantom{11111} The type of the item\\
\textbf{Return type:}\\
\phantom{11111}  \texttt{ItemType\_T*}

\vskip 0.5cm
\begin{center}
	\noindent\rule{4cm}{1.2pt}
\end{center}
\vskip 0.7cm

\textit{function} \texttt{cycles(worker, num\_cycles)}\\
\phantom{11111} Perform the given amount of iterations in the Worker\\
\textbf{Arguments:}\\
\phantom{11111} \texttt{worker} - The Worker whos items should execute work\\
\phantom{11111} \texttt{num\_cycles} - The number of iterations to execute\\
\textbf{Return type:}\\
\phantom{11111}  void\\
\textbf{Variations:}\\
\phantom{11111} \texttt{cycle(worker, do\_work)}\\
\phantom{11111} \phantom{11111} Perform a single iteration. The second parameter is a boolean indicating whether work should be performed. If no work is performed, communication routines are still executed as in regular iterations.
\newpage


\textit{function} \texttt{iterate\_table(worker, table\_key)}\\
\phantom{11111} Get all items stored within a table\\
\textbf{Arguments:}\\
\phantom{11111} \texttt{worker} - The Worker containing the given table\\
\phantom{11111} \texttt{table\_key} - The table to get items from\\
\textbf{Template arguments:}\\
\phantom{11111} \texttt{<class ItemType\_T>}\\
\textbf{Return type:}\\
\phantom{11111}  \texttt{Robin\_Map<ItemKey\_T, ItemType\_T>}\\
\phantom{11111}\phantom{11111} The hash table used in Saddlebags, with a STL-compliant iterator.

\begin{center}
	\noindent\rule{4cm}{1.2pt}
\end{center}

\textit{enum} \texttt{SendingMode}\\
\phantom{11111} Enumerable with values for the available sending modes;
\texttt{Combining},
\texttt{Direct},
\texttt{GasnetBuffering} and \texttt{Buffering}

\begin{center}
	\noindent\rule{4cm}{1.2pt}
\end{center}

\textit{class} \texttt{Item}\\
\textbf{Template parameters:}\\
\phantom{11111} \texttt{template<class TableKey\_T, class ItemKey\_T, class Msg\_T>}\\
\textbf{Methods:}\\
\phantom{11111} \texttt{virtual void on\_create()}\\
\phantom{11111} \texttt{virtual void refresh()}\\
\phantom{11111} \texttt{virtual void foreign\_push()}\\
\phantom{11111} \texttt{virtual Msg\_T foreign\_pull(int tag)}\\
\phantom{11111} \texttt{virtual void returning\_pull(Message returning\_message)}\\
\phantom{11111} \texttt{virtual void do\_work()}\\
\phantom{11111} \texttt{virtual void finishing\_work()}\\
\phantom{11111} \texttt{void pull(TableKey\_T destTableKey, ItemKey\_T destItemKey, int tag)}
\phantom{11111} \texttt{void push(TableKey\_T destTableKey, ItemKey\_T destItemKey, Msg\_T val)}
The significance of these methods are explained in section \ref{section:events}
\newpage
\textit{class} \texttt{Worker}\\
\textbf{Template parameters:}\\
\phantom{11111} \texttt{template<class TableKey\_T, class ItemKey\_T, class Msg\_T>}\\
\textbf{Methods:}\\
\phantom{11111} \texttt{void set\_mode(SendingMode mode)}\\
\phantom{11111}\phantom{11111} Set the sending mode of this Worker\\\\
\phantom{11111} \texttt{void set\_replication\_level(int level)}\\
\phantom{11111}\phantom{11111} Set the degree of replication in this worker. This will overwrite existing replicas and it is recommended to set the replication level before any iterations are started\\\\
\phantom{11111} \texttt{bool check\_if\_partition(TableKey\_T table\_key, ItemKey\_T obj)}\\
\phantom{11111}\phantom{11111} Returns true if the given Item should be placed in this rank with the current distribution model\\\\
\phantom{11111} \texttt{int get\_partition(TableKey\_T table\_key, ItemKey\_T obj)}\\
\phantom{11111}\phantom{11111} Return the rank that the given Item should be placed in with the current distribution model\\\\

% End of document
\end{document}
\grid
\grid




































