#!/bin/bash
# spark-connect.batch - sbatch script to connect to Spark stand-alone cluster started with SLURM
# takes as input spark URL to master node and (optionally) SLURM jobid of the spark cluster

#SBATCH --account=nn9342k
#SBATCH --nodes=16
#  ntasks per node MUST be one, because multiple slaves per work doesn't
#  work well with slurm + spark in this script (they would need increasing
#  ports among other things)
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=8G
#SBATCH --time=01:00:00

module purge
module load spark/2.0.2-bin-hadoop2.7
module load python2/2.7.10

if [ -z "$1" ]; then
    echo Usage: $0 SPARK_MASTER [SPARK_MASTER_JOBID]
    exit 1
fi

SPARK_MASTER=$1

$SPARK_HOME/bin/spark-submit sparkpagerank.py --master $SPARK_MASTER --properties-file $SPARK_CONFIG

if [ "$2" ]; then
    echo Finished, stopping Spark Cluster, jobid "$2"
    scancel $2
fi

